{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a01c9d4-8060-41ee-b5b7-79749d30d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfa539-c79f-4e0d-a83a-2e1359ee7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cnn_article = pd.read_parquet(\"../Article/cnn_daily_news/cnn_data/train_article_cnn_5000.parquet\")\n",
    "df_val_cnn_article = pd.read_parquet(\"../Article/cnn_daily_news/cnn_data/val_article_cnn_1000.parquet\")\n",
    "df_test_cnn_article = pd.read_parquet(\"../Article/cnn_daily_news/cnn_data/test_article_cnn_1000.parquet\")\n",
    "df_train_arxiv_paper = pd.read_parquet(\"../arxiv_research/arxiv_data/train_article_arxiv_5000.parquet\")\n",
    "df_val_arxiv_paper = pd.read_parquet(\"../arxiv_research/arxiv_data/val_article_arxiv_1000.parquet\")\n",
    "df_test_arxiv_paper = pd.read_parquet(\"../arxiv_research/arxiv_data/test_article_arxiv_1000.parquet\")\n",
    "df_train_email = pd.read_parquet(\"../enron_email/enron_train_5000.parquet\")\n",
    "df_val_email = pd.read_parquet(\"../enron_email/enron_val_1000.parquet\")\n",
    "df_test_email = pd.read_parquet(\"../enron_email/enron_test_1000.parquet\")\n",
    "df_train_reddit_post = pd.read_parquet(\"../reddit/reddit_train_5000.parquet\")\n",
    "df_val_reddit_post = pd.read_parquet(\"../reddit/reddit_val_1000.parquet\")\n",
    "df_test_reddit_post = pd.read_parquet(\"../reddit/reddit_test_1000.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b4a28f6-b4fe-4cba-9efe-827e424cbaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96b7c7a090c426baeb4db634b85443e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"./mistral_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16277a-0c63-44f8-82d1-d9bf688e9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mistral_article(text: str) -> str:\n",
    "\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an expert news editor. Your goal is to produce a clear, objective, \"\n",
    "            \"journalistic-style briefing. You must strictly follow formatting rules.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Summarize the following article in EXACTLY six to seven complete sentences. \"\n",
    "            \"This is mandatory. Do not generate fewer or more than seven sentences.\\n\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"- Journalistic briefing tone (Inverted Pyramid style)\\n\" \n",
    "            \"- No bullet points, no lists\\n\"\n",
    "            \"- No citations or references\\n\"\n",
    "            \"- No markdown formatting\\n\"\n",
    "            \"- Do not copy any sentence from the article; rephrase everything\\n\\n\"\n",
    "            f\"Article:\\n{text}\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=32000,\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=800,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    gen_ids = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    summary = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    del inputs, outputs, gen_ids\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  \n",
    "        \n",
    "    return summary\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf0c1e-9ba1-482a-b172-8fb3599225aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "out_path = Path(\"train_article_cnn_summaries_mistral_v03_v2.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_train_cnn_article[\"article\"]):\n",
    "    if( i % 100 ==0):\n",
    "        print(i)\n",
    "    summary = summarize_mistral_article(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"cnn_daily_news\",\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2dcbb-81c5-44f6-8ff9-9a598c139ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(\"val_article_cnn_summaries_mistral_v03_v2.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_val_cnn_article[\"article\"]):\n",
    "    if( i % 100 ==0):\n",
    "        print(i)\n",
    "    summary = summarize_mistral_article(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"cnn_daily_news\",\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d8a0f8-281f-46f2-89a0-d64f37e588ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(\"test_article_cnn_summaries_mistral_v03_v2.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_test_cnn_article[\"article\"]):\n",
    "    if( i % 100 ==0):\n",
    "        print(i)\n",
    "    summary = summarize_mistral_article(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"cnn_daily_news\",\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b839be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mistral_research(text: str) -> str:\n",
    "\n",
    "\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an expert summarizer. Your goal is to produce a clear, factual, \"\n",
    "            \"abstract-style summary. You must strictly follow formatting rules.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Summarize the following article in EXACTLY six to seven complete sentences. \"\n",
    "            \"This is mandatory. Do not generate fewer or more than seven sentences.\\n\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"- Abstract-style academic writing\\n\"\n",
    "            \"- No bullet points, no lists\\n\"\n",
    "            \"- No citations or references\\n\"\n",
    "            \"- No markdown formatting\\n\"\n",
    "            \"- Do not copy any sentence from the article; rephrase everything\\n\\n\"\n",
    "            f\"Article:\\n{text}\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=32000,\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=800,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    gen_ids = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    summary = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4663490-41e4-4f62-b693-944327ac4a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "out_path = Path(\"train_article_arxiv_summaries_mistral_v03.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_train_arxiv_paper[\"article\"]):\n",
    "    if( i % 100 ==0):\n",
    "        print(i)\n",
    "    summary = summarize_mistral_research(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"arxiv\",\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81f61a2b-2505-4478-b854-9c287b3f8388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to val_article_arxiv_summaries_mistral_v03.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"val_article_arxiv_summaries_mistral_v03.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_val_arxiv_paper[\"article\"]):\n",
    "    if( i % 100 ==0):\n",
    "        print(i)\n",
    "    summary = summarize_mistral_research(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"arxiv\",\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1e85c64-c1c6-4447-a09f-4d75a0b0ad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to test_article_arxiv_summaries_mistral_v03.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"test_article_arxiv_summaries_mistral_v03.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_test_arxiv_paper[\"article\"]):\n",
    "    if( i % 100 ==0):\n",
    "        print(i)\n",
    "    summary = summarize_mistral_research(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"arxiv\",\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f470bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_reddit_mistral(text: str) -> str:\n",
    "\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an expert at summarizing Reddit posts. Your goal is to provide a clear, \"\n",
    "            \"neutral summary of the user's post while keeping the tone natural and conversational. \"\n",
    "            \"Capture the key situation, context, motivations, and main concerns. Avoid adding \"\n",
    "            \"opinions or judgments.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Summarize the following Reddit post in 4–6 sentences. \"\n",
    "            \"Keep the summary concise, neutral, and easy to understand.\\n\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"- Use plain, natural language\\n\"\n",
    "            \"- Reflect the poster's situation accurately\\n\"\n",
    "            \"- Include the key events, motivations, and concerns\\n\"\n",
    "            \"- No bullet points, no lists\\n\"\n",
    "            \"- No advising, judging, or moralizing\\n\"\n",
    "            \"- Do not copy text; fully rephrase everything\\n\\n\"\n",
    "            f\"Reddit Post:\\n{text}\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=32000,\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=800,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    gen_ids = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    summary = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4286b154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5000 rows to train_reddit_summaries_mistral_v03.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"train_reddit_summaries_mistral_v03.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_train_reddit_post[\"text\"]):\n",
    "    if( i % 100 ==0):\n",
    "        print(i)\n",
    "    summary = summarize_reddit_mistral(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"reddit\",\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29fed9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to val_reddit_summaries_mistral_v03.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"val_reddit_summaries_mistral_v03.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_val_reddit_post[\"text\"]):\n",
    "    if( i % 100 ==0):\n",
    "        print(i)\n",
    "    summary = summarize_reddit_mistral(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"reddit\",\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42f25fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to test_reddit_summaries_mistral_v03.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"test_reddit_summaries_mistral_v03.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_test_reddit_post[\"text\"]):\n",
    "    if( i % 100 ==0):\n",
    "        print(i)\n",
    "    summary = summarize_reddit_mistral(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"reddit\",\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51235693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_email_mistral(text: str) -> str:\\\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You summarize business emails in a concise, neutral, and professional tone. \"\n",
    "                \"Your summaries must begin immediately with the content, without any introductory phrases, \"\n",
    "                \"explanations, or meta-commentary.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Write a summary of the following email in 3–5 complete sentences.\\n\\n\"\n",
    "                \"Requirements:\\n\"\n",
    "                \"- Begin the summary immediately with the content; do NOT write phrases such as \"\n",
    "                \"  'Here is a summary', 'The email says', or 'This message discusses'\\n\"\n",
    "                \"- Professional tone\\n\"\n",
    "                \"- Capture the core purpose, key points, and any actions or decisions\\n\"\n",
    "                \"- No bullet points or lists\\n\"\n",
    "                \"- No greetings or signatures\\n\"\n",
    "                \"- No quoting, copying, or references to the fact that this is an email\\n\"\n",
    "                \"- Ignore headers, footers, and forward chains unless essential\\n\\n\"\n",
    "                f\"Email:\\n{text}\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=32000,\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=800,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    gen_ids = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    summary = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10fe9315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5000 rows to train_email_summaries_mistral_v03.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"train_email_summaries_mistral_v03.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_train_email[\"text\"]):\n",
    "    if( i % 100 ==0):\n",
    "        print(i)\n",
    "    summary = summarize_email_mistral(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"email\",\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a64a99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to val_email_summaries_mistral_v03.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"val_email_summaries_mistral_v03.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_val_email[\"text\"]):\n",
    "    if( i % 100 ==0):\n",
    "        print(i)\n",
    "    summary = summarize_email_mistral(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"email\",\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57a789c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to test_email_summaries_mistral_v03.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"test_email_summaries_mistral_v03.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_test_email[\"text\"]):\n",
    "    if( i % 100 ==0):\n",
    "        print(i)\n",
    "    summary = summarize_email_mistral(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"email\",\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
