{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a01c9d4-8060-41ee-b5b7-79749d30d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfa539-c79f-4e0d-a83a-2e1359ee7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cnn_article = pd.read_parquet(\"../Article/cnn_daily_news/cnn_data/train_article_cnn_5000.parquet\")\n",
    "df_val_cnn_article = pd.read_parquet(\"../Article/cnn_daily_news/cnn_data/val_article_cnn_1000.parquet\")\n",
    "df_test_cnn_article = pd.read_parquet(\"../Article/cnn_daily_news/cnn_data/test_article_cnn_1000.parquet\")\n",
    "df_train_arxiv_paper = pd.read_parquet(\"../arxiv_research/arxiv_data/train_article_arxiv_5000.parquet\")\n",
    "df_val_arxiv_paper = pd.read_parquet(\"../arxiv_research/arxiv_data/val_article_arxiv_1000.parquet\")\n",
    "df_test_arxiv_paper = pd.read_parquet(\"../arxiv_research/arxiv_data/test_article_arxiv_1000.parquet\")\n",
    "df_train_email = pd.read_parquet(\"../enron_email/enron_train_5000.parquet\")\n",
    "df_val_email = pd.read_parquet(\"../enron_email/enron_val_1000.parquet\")\n",
    "df_test_email = pd.read_parquet(\"../enron_email/enron_test_1000.parquet\")\n",
    "df_train_reddit_post = pd.read_parquet(\"../reddit/reddit_train_5000.parquet\")\n",
    "df_val_reddit_post = pd.read_parquet(\"../reddit/reddit_val_1000.parquet\")\n",
    "df_test_reddit_post = pd.read_parquet(\"../reddit/reddit_test_1000.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a28f6-b4fe-4cba-9efe-827e424cbaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75336a4fb35b4950ae87e52bfb963c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_hf =\"\" #change this\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token = token_hf)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=token_hf,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"./llama_model\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "836a7e7f-206c-43e0-955b-5afd5bf15b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_llama3_article(text: str) -> str:\n",
    "\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an expert news editor. Your goal is to produce a clear, objective, \"\n",
    "            \"journalistic-style briefing. You must strictly follow formatting rules.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Summarize the following article in EXACTLY six to seven complete sentences. \"\n",
    "            \"This is mandatory. Do not generate fewer or more than seven sentences.\\n\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"- Journalistic briefing tone (Inverted Pyramid style)\\n\" \n",
    "            \"- No bullet points, no lists\\n\"\n",
    "            \"- No citations or references\\n\"\n",
    "            \"- No markdown formatting\\n\"\n",
    "            \"- Do not copy any sentence from the article; rephrase everything\\n\\n\"\n",
    "            f\"Article:\\n{text}\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=32000,\n",
    "    ).to(model.device)\n",
    "\n",
    "    eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") or tokenizer.eos_token_id\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=800,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=eot_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    gen_ids = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    summary = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59cf0c1e-9ba1-482a-b172-8fb3599225aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "Saved 5000 rows to train_article_cnn_summaries_llama31_v2.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "out_path = Path(\"train_article_cnn_summaries_llama31_v2.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_train_cnn_article[\"article\"]):\n",
    "    if ( i % 100 == 0):\n",
    "        print(i)\n",
    "    summary = summarize_llama3_article(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"cnn_daily_news\",\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbd2dcbb-81c5-44f6-8ff9-9a598c139ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to val_article_cnn_summaries_llama31_v2.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"val_article_cnn_summaries_llama31_v2.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_val_cnn_article[\"article\"]):\n",
    "    summary = summarize_llama3_article(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"cnn_daily_news\",\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d8a0f8-281f-46f2-89a0-d64f37e588ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to test_article_cnn_summaries_llama31_v2.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"test_article_cnn_summaries_llama31_v2.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_test_cnn_article[\"article\"]):\n",
    "    summary = summarize_llama3_article(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"cnn_daily_news\",\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b839be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_llama3_research(text: str) -> str:\n",
    "\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an expert summarizer. Your goal is to produce a clear, factual, \"\n",
    "            \"abstract-style summary. You must strictly follow formatting rules.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Summarize the following article in EXACTLY six to seven complete sentences. \"\n",
    "            \"This is mandatory. Do not generate fewer or more than seven sentences.\\n\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"- Abstract-style academic writing\\n\"\n",
    "            \"- No bullet points, no lists\\n\"\n",
    "            \"- No citations or references\\n\"\n",
    "            \"- No markdown formatting\\n\"\n",
    "            \"- Do not copy any sentence from the article; rephrase everything\\n\\n\"\n",
    "            f\"Article:\\n{text}\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=32000,\n",
    "    ).to(model.device)\n",
    "\n",
    "    eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") or tokenizer.eos_token_id\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=800,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=eot_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    gen_ids = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    summary = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4663490-41e4-4f62-b693-944327ac4a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5000 rows to train_article_arxiv_summaries_llama31.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "out_path = Path(\"train_article_arxiv_summaries_llama31.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_train_arxiv_paper[\"article\"]):\n",
    "    summary = summarize_llama3_research(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"arxiv\",\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81f61a2b-2505-4478-b854-9c287b3f8388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to val_article_arxiv_summaries_llama31.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"val_article_arxiv_summaries_llama31.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_val_arxiv_paper[\"article\"]):\n",
    "    summary = summarize_llama3_research(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"arxiv\",\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1e85c64-c1c6-4447-a09f-4d75a0b0ad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to test_article_arxiv_summaries_llama31.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"test_article_arxiv_summaries_llama31.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_test_arxiv_paper[\"article\"]):\n",
    "    summary = summarize_llama3_research(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"arxiv\",\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88f25393-7a5e-4796-b302-37cc8da24b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_reddit_llama3(text: str) -> str:\n",
    "\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an expert at summarizing Reddit posts. Your goal is to provide a clear, \"\n",
    "            \"neutral summary of the user's post while keeping the tone natural and conversational. \"\n",
    "            \"Capture the key situation, context, motivations, and main concerns. Avoid adding \"\n",
    "            \"opinions or judgments.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Summarize the following Reddit post in 4–6 sentences. \"\n",
    "            \"Keep the summary concise, neutral, and easy to understand.\\n\\n\"\n",
    "            \"Requirements:\\n\"\n",
    "            \"- Use plain, natural language\\n\"\n",
    "            \"- Reflect the poster's situation accurately\\n\"\n",
    "            \"- Include the key events, motivations, and concerns\\n\"\n",
    "            \"- No bullet points, no lists\\n\"\n",
    "            \"- No advising, judging, or moralizing\\n\"\n",
    "            \"- Do not copy text; fully rephrase everything\\n\\n\"\n",
    "            f\"Reddit Post:\\n{text}\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=32000,\n",
    "    ).to(model.device)\n",
    "\n",
    "    eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") or tokenizer.eos_token_id\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=800,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=eot_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    gen_ids = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    summary = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17bdae0d-8e16-4cfe-88cc-ae3ac615af1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5000 rows to train_reddit_summaries_llama31.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"train_reddit_summaries_llama31.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_train_reddit_post[\"text\"]):\n",
    "    summary = summarize_reddit_llama3(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"reddit\",\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c77a1e5-a7d0-4523-9d64-fad179c609cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to val_reddit_summaries_llama31.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"val_reddit_summaries_llama31.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_val_reddit_post[\"text\"]):\n",
    "    summary = summarize_reddit_llama3(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"reddit\",\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "853be04e-4019-4030-8fab-a93490e02960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to test_reddit_summaries_llama31.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"test_reddit_summaries_llama31.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_test_reddit_post[\"text\"]):\n",
    "    summary = summarize_reddit_llama3(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"reddit\",\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e1018-2853-4694-96eb-8b1074181d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_email_llama3(text: str) -> str:\\\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You summarize business emails in a concise, neutral, and professional tone. \"\n",
    "                \"Your summaries must begin immediately with the content, without any introductory phrases, \"\n",
    "                \"explanations, or meta-commentary.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Write a summary of the following email in 3–5 complete sentences.\\n\\n\"\n",
    "                \"Requirements:\\n\"\n",
    "                \"- Begin the summary immediately with the content; do NOT write phrases such as \"\n",
    "                \"  'Here is a summary', 'The email says', or 'This message discusses'\\n\"\n",
    "                \"- Professional tone\\n\"\n",
    "                \"- Capture the core purpose, key points, and any actions or decisions\\n\"\n",
    "                \"- No bullet points or lists\\n\"\n",
    "                \"- No greetings or signatures\\n\"\n",
    "                \"- No quoting, copying, or references to the fact that this is an email\\n\"\n",
    "                \"- Ignore headers, footers, and forward chains unless essential\\n\\n\"\n",
    "                f\"Email:\\n{text}\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=32000,\n",
    "    ).to(model.device)\n",
    "\n",
    "    eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") or tokenizer.eos_token_id\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=800,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=eot_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    gen_ids = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    summary = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6da00cce-02a3-45e3-8d82-3cd69f64686d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5000 rows to train_email_summaries_llama31.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"train_email_summaries_llama31.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_train_email[\"text\"]):\n",
    "    summary = summarize_email_llama3(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"email\",\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af91859-cda1-41a9-9d59-c9c7849c6178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to val_email_summaries_llama31.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"val_email_summaries_llama31.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_val_email[\"text\"]):\n",
    "    summary = summarize_email_llama3(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"email\",\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b15dd76-c1f3-446b-b9c0-b677ee0c2f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 rows to test_email_summaries_llama31.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = Path(\"test_email_summaries_llama31.csv\")\n",
    "rows = []\n",
    "\n",
    "for i, text in enumerate(df_test_email[\"text\"]):\n",
    "    summary = summarize_email_llama3(text)   \n",
    "    rows.append({\n",
    "        \"doc_id\": i,                   \n",
    "        \"text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"dataset\": \"email\",\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(rows, columns=[\"doc_id\", \"text\", \"summary\",\"dataset\", \"model\"])\n",
    "df_out.to_csv(out_path, index=False)\n",
    "print(f\"Saved {len(df_out)} rows to {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
