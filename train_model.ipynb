{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb                                   \n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,precision_recall_fscore_support\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"LLM-Authorship-Attribution\", name=\"RoBERTa-Master-Run-optimized\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT = \"roberta-large\" \n",
    "MAX_LEN = 512\n",
    "OUTPUT_DIR = \"./my_finetuned_models/roberta_author_attribution_all_domain_optimized\"\n",
    "CACHE_DIR = \"./hf_cache\" \n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"Llama-3.1\": 0,\n",
    "    \"Qwen-2.5\": 1,\n",
    "    \"Mistral-v0.3\": 2,\n",
    "    \"Granite-3.3\": 3,\n",
    "    \"GLM-4\": 4\n",
    "\n",
    "}\n",
    "\n",
    "num_labels = len(LABEL_MAP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcdb9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Load Data\")\n",
    "df_train_full = pd.read_csv(\"../all_domain_data/train_combined.csv\") \n",
    "df_val_full   = pd.read_csv(\"../all_domain_data/val_combined.csv\")\n",
    "df_test_full  = pd.read_csv(\"../all_domain_data/test_combined.csv\")\n",
    "\n",
    "def fix_columns(df):\n",
    "    if 'text' in df.columns:\n",
    "        df.rename(columns={'text': 'source_text'}, inplace=True)\n",
    "\n",
    "    if 'summary' in df.columns:\n",
    "        df.rename(columns={'summary': 'text'}, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_train_full = fix_columns(df_train_full)\n",
    "df_val_full   = fix_columns(df_val_full)\n",
    "df_test_full  = fix_columns(df_test_full)\n",
    "\n",
    "def clean_data(df):\n",
    "    df = df.dropna(subset=['text']).copy() \n",
    "    df['text'] = df['text'].astype(str)\n",
    "\n",
    "    df = df[df['text'].str.strip() != \"\"].copy()\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Cleaning data...\")\n",
    "df_train_full = clean_data(df_train_full)\n",
    "df_val_full   = clean_data(df_val_full)\n",
    "df_test_full  = clean_data(df_test_full)\n",
    "\n",
    "def encode_label(model_name):\n",
    "    for key, val in LABEL_MAP.items():\n",
    "        if key.split(\"-\")[0].lower() in str(model_name).lower(): \n",
    "            return val\n",
    "    return -1 \n",
    "\n",
    "df_train_full['label'] = df_train_full['model'].apply(encode_label)\n",
    "df_val_full['label']   = df_val_full['model'].apply(encode_label)\n",
    "df_test_full['label']  = df_test_full['model'].apply(encode_label)\n",
    "\n",
    "df_train_full = df_train_full[df_train_full['label'] != -1].copy()\n",
    "df_val_full   = df_val_full[df_val_full['label'] != -1].copy()\n",
    "df_test_full  = df_test_full[df_test_full['label'] != -1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca168561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_CHECKPOINT, \n",
    "    cache_dir=CACHE_DIR \n",
    ")\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "hf_train = Dataset.from_pandas(df_train_full)\n",
    "hf_val   = Dataset.from_pandas(df_val_full)\n",
    "hf_test  = Dataset.from_pandas(df_test_full)\n",
    "\n",
    "tokenized_train = hf_train.map(preprocess_function, batched=True)\n",
    "tokenized_val   = hf_val.map(preprocess_function, batched=True)\n",
    "tokenized_test  = hf_test.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train with wanb\n",
    "print(f\"\\n--- 4. Loading Model: {MODEL_CHECKPOINT} ---\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT, \n",
    "    num_labels=num_labels,\n",
    "    cache_dir=CACHE_DIR \n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    #  Calculate Accuracy\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Calculate Precision, Recall, F1 (Macro average)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro'\n",
    "    )\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"roberta-run\",\n",
    "    learning_rate=3.85e-5,  \n",
    "    per_device_train_batch_size=32, \n",
    "    per_device_eval_batch_size=64,  \n",
    "    weight_decay=0.2,            \n",
    "    num_train_epochs=5,          \n",
    "    warmup_ratio=0.1,            \n",
    "    bf16=True,                      \n",
    "    tf32=True,                      \n",
    "    dataloader_num_workers=4,       \n",
    "\n",
    "    \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=f'{OUTPUT_DIR}/logs',\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"Starting Training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b5c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "preds_output = trainer.predict(tokenized_test)\n",
    "y_pred = np.argmax(preds_output.predictions, axis=1)\n",
    "y_true = preds_output.label_ids\n",
    "\n",
    "final_acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"FINAL ACCURACY: {final_acc:.4f}\")\n",
    "\n",
    "# Log final confusion matrix to W&B\n",
    "wandb.log({\"test_accuracy\": final_acc})\n",
    "wandb.sklearn.plot_confusion_matrix(y_true, y_pred, list(LABEL_MAP.keys()))\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
