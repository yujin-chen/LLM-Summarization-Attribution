{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05777b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"./my_finetuned_models/roberta_author_attribution_all_domain_optimized/checkpoint-9375\"\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"Llama-3.1\": 0,\n",
    "    \"Qwen-2.5\": 1,\n",
    "    \"Mistral-v0.3\": 2,\n",
    "    \"Granite-3.3\": 3,\n",
    "    \"GLM-4\": 4\n",
    "}\n",
    "ID2LABEL = {v: k for k, v in LABEL_MAP.items()}\n",
    "\n",
    "df_test = pd.read_csv(\"../all_domain_data/test_combined.csv\")\n",
    "\n",
    "# Standardize\n",
    "if 'text' in df_test.columns: df_test.rename(columns={'text': 'source_text'}, inplace=True)\n",
    "if 'summary' in df_test.columns: df_test.rename(columns={'summary': 'text'}, inplace=True)\n",
    "if 'dataset' in df_test.columns: df_test.rename(columns={'dataset': 'domain'}, inplace=True)\n",
    "\n",
    "df_test = df_test.dropna(subset=['text']).copy()\n",
    "df_test['text'] = df_test['text'].astype(str)\n",
    "\n",
    "# Map Labels\n",
    "def encode_label(model_name):\n",
    "    name = str(model_name).lower()\n",
    "    if \"llama\" in name: return 0\n",
    "    if \"qwen\" in name: return 1\n",
    "    if \"mistral\" in name: return 2\n",
    "    if \"granite\" in name: return 3\n",
    "    if \"glm\" in name: return 4\n",
    "    return -1\n",
    "\n",
    "df_test['label'] = df_test['model'].apply(encode_label)\n",
    "df_test = df_test[df_test['label'] != -1].copy()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "hf_test = Dataset.from_pandas(df_test)\n",
    "tokenized_test = hf_test.map(preprocess, batched=True)\n",
    "tokenized_test = tokenized_test.remove_columns([c for c in tokenized_test.column_names if c not in ['input_ids', 'attention_mask', 'label']])\n",
    "tokenized_test.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "trainer = Trainer(model=model, data_collator=data_collator)\n",
    "preds = trainer.predict(tokenized_test)\n",
    "\n",
    "y_pred = np.argmax(preds.predictions, axis=1)\n",
    "y_true = preds.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a76cbbf-3b13-452d-ac21-c692051e229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "tick_labels = [ID2LABEL[i] for i in range(len(ID2LABEL))]\n",
    "cm_norm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_norm,\n",
    "    display_labels=tick_labels\n",
    ")\n",
    "disp.plot(\n",
    "    cmap='Blues', \n",
    "    ax=ax, \n",
    "    values_format='.2f', \n",
    "    colorbar=True\n",
    ")\n",
    "\n",
    "for labels in disp.text_.ravel():\n",
    "    labels.set_color(\"black\")\n",
    "    labels.set_fontsize(12)\n",
    "    labels.set_weight(\"bold\")\n",
    "\n",
    "plt.title(f'(Acc: {acc:.2%})')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./all_domain_analysis_optimized/Figure1_Final_Robust.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83faad82-1fcb-4abb-acd2-f406dd333a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length analysis\n",
    "\n",
    "if 'pred_label' not in df_test.columns:\n",
    "    df_test['pred_label'] = y_pred \n",
    "\n",
    "if 'is_correct' not in df_test.columns:\n",
    "    df_test['is_correct'] = df_test['label'] == df_test['pred_label']\n",
    "\n",
    "# Calculate Lengths, token counts\n",
    "df_test['token_count'] = df_test['text'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "# Create Bins (Groups)\n",
    "bins = [0, 100, 200, 300, 400, 500, 1000]\n",
    "labels = ['0-100', '100-200', '200-300', '300-400', '400-500', '500+']\n",
    "df_test['len_bin'] = pd.cut(df_test['token_count'], bins=bins, labels=labels)\n",
    "\n",
    "# Calculate Accuracy per Bin\n",
    "bin_acc = df_test.groupby('len_bin', observed=True)['is_correct'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = bin_acc.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('All Domain Attribution Accuracy vs. Summary Length')\n",
    "plt.xlabel('Summary Length (Tokens)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add numbers on top of bars\n",
    "for p in bars.patches:\n",
    "    bars.annotate(f'{p.get_height():.2f}', \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, 9), \n",
    "                   textcoords = 'offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./all_domain_analysis_optimized/Figure2_LengthAnalysis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84789938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add Labels\n",
    "df_test['true_label_id'] = y_true\n",
    "df_test['pred_label_id'] = y_pred\n",
    "df_test['prediction_name'] = df_test['pred_label_id'].map(ID2LABEL)\n",
    "\n",
    "df_test['is_correct'] = df_test['true_label_id'] == df_test['pred_label_id']\n",
    "probs = torch.nn.functional.softmax(torch.tensor(preds.predictions), dim=-1).numpy()\n",
    "df_test['confidence_score'] = np.max(probs, axis=1)\n",
    "\n",
    "output_filename = \"./all_domain_analysis_optimized/test_results_with_predictions.csv\"\n",
    "df_test.to_csv(output_filename, index=False)\n",
    "\n",
    "# Isolate & Save Errors\n",
    "df_errors = df_test[~df_test['is_correct']].copy()\n",
    "df_errors = df_errors.sort_values('confidence_score', ascending=False)\n",
    "\n",
    "error_filename = \"./all_domain_analysis_optimized/wrong_predictions_only.csv\"\n",
    "df_errors.to_csv(error_filename, index=False)\n",
    "\n",
    "# Quick Sample\n",
    "if len(df_errors) > 0:\n",
    "    top_error = df_errors.iloc[0]\n",
    "    print(\"\\n--- Most Common Mistake ---\")\n",
    "    print(f\"True: {top_error['model']} | Pred: {top_error['prediction_name']}\")\n",
    "    print(f\"Confidence: {top_error['confidence_score']:.4f}\")\n",
    "    print(f\"Text: {str(top_error['text'])[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_full = pd.read_csv(\"./all_domain_analysis_optimized/test_results_with_predictions.csv\").dropna(subset=['text'])\n",
    "df_errors = pd.read_csv(\"./all_domain_analysis_optimized/wrong_predictions_only.csv\").dropna(subset=['text'])\n",
    "\n",
    "df_full['text'] = df_full['text'].astype(str).str.lower()\n",
    "df_errors['text'] = df_errors['text'].astype(str).str.lower()\n",
    "\n",
    "LABEL_ORDER = [\"Llama-3.1\", \"Qwen-2.5\", \"Mistral-v0.3\", \"Granite-3.3\", \"GLM-4\"]\n",
    "TICK_LABELS = LABEL_ORDER\n",
    "\n",
    "# Error Heat map\n",
    "cm_errors = confusion_matrix(\n",
    "    df_errors['label'], \n",
    "    df_errors['pred_label_id'], \n",
    "    labels=range(len(TICK_LABELS))\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_errors, display_labels=TICK_LABELS)\n",
    "disp.plot(cmap='Reds', ax=ax, values_format='d', colorbar=True)\n",
    "\n",
    "# Styling\n",
    "for labels in disp.text_.ravel():\n",
    "    labels.set_color(\"black\")\n",
    "    labels.set_fontsize(12)\n",
    "    if labels.get_text() != '0': labels.set_weight(\"bold\")\n",
    "\n",
    "plt.title('All Domain Heatmap of Misclassifications (Counts)')\n",
    "plt.ylabel('True Model (Actual)')\n",
    "plt.xlabel('Predicted Model (The Mistake)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./all_domain_analysis_optimized/Error_Heatmap.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment if data is not in memory\n",
    "# df_full = pd.read_csv(\"test_results_with_predictions.csv\").dropna(subset=['text'])\n",
    "# df_errors = pd.read_csv(\"wrong_predictions_only.csv\").dropna(subset=['text'])\n",
    "# df_full['text'] = df_full['text'].astype(str).str.lower()\n",
    "# df_errors['text'] = df_errors['text'].astype(str).str.lower()\n",
    "\n",
    "# Deceptive Words\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Top 10 'Deceptive Words' per model\")\n",
    "print(\"(When this model is misclassified, these words appear most)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name in df_errors['model'].unique():\n",
    "    subset = df_errors[df_errors['model'] == model_name]\n",
    "    \n",
    "    if len(subset) < 1: continue\n",
    "        \n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Total Errors: {len(subset)}\")\n",
    "    \n",
    "    if 'prediction_name' in subset.columns:\n",
    "        top_impostor = subset['prediction_name'].mode()[0]\n",
    "        print(f\"Most often confused with: {top_impostor}\")\n",
    "\n",
    "    try:\n",
    "        vectorizer = CountVectorizer(stop_words='english', max_features=10, ngram_range=(1, 2))\n",
    "        X = vectorizer.fit_transform(subset['text'])\n",
    "        word_counts = np.asarray(X.sum(axis=0)).flatten()\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        sorted_idx = word_counts.argsort()[::-1]\n",
    "        \n",
    "        print(\"Top 10 Trigger Words:\")\n",
    "        for i in range(len(feature_names)):\n",
    "            idx = sorted_idx[i]\n",
    "            print(f\"  {i+1}. {feature_names[idx]} ({word_counts[idx]})\")\n",
    "            \n",
    "    except ValueError:\n",
    "        print(\"Not enough text data to analyze keywords\")\n",
    "        \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# Word Frequency count\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Top 10 Most Frequent Words Per Model (all samples)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name in df_full['model'].unique():\n",
    "    subset = df_full[df_full['model'] == model_name]\n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=10, ngram_range=(1, 2), max_df=0.9)\n",
    "    \n",
    "    try:\n",
    "        X = vectorizer.fit_transform(subset['text'])\n",
    "        word_counts = np.asarray(X.sum(axis=0)).flatten()\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        sorted_idx = word_counts.argsort()[::-1]\n",
    "        \n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        for i in range(len(feature_names)):\n",
    "            idx = sorted_idx[i]\n",
    "            print(f\"  {i+1}. {feature_names[idx]} ({word_counts[idx]})\")\n",
    "            \n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d500e",
   "metadata": {},
   "source": [
    "### Analysis By Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311be82d-a13f-4fd3-a40a-a9b4b0c9834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_folder = \"domain_analysis_plots\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    df_full = pd.read_csv(\"test_results_with_predictions.csv\").dropna(subset=['text'])\n",
    "    df_errors = pd.read_csv(\"wrong_predictions_only.csv\").dropna(subset=['text'])\n",
    "    \n",
    "    # Normalize text\n",
    "    df_full['text'] = df_full['text'].astype(str).str.lower()\n",
    "    df_errors['text'] = df_errors['text'].astype(str).str.lower()\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Files not found.\")\n",
    "    exit()\n",
    "\n",
    "LABEL_ORDER = [\"Llama-3.1\", \"Qwen-2.5\", \"Mistral-v0.3\", \"Granite-3.3\", \"GLM-4\"]\n",
    "TICK_LABELS = LABEL_ORDER\n",
    "\n",
    "# Loop data analysis for each specific domain\n",
    "unique_domains = df_full['domain'].unique()\n",
    "\n",
    "for domain in unique_domains:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Processing Domain: {domain.upper()}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Filter Data\n",
    "    domain_full = df_full[df_full['domain'] == domain]\n",
    "    domain_errors = df_errors[df_errors['domain'] == domain]\n",
    "    \n",
    "    if len(domain_full) == 0: continue\n",
    "\n",
    "    #Accuracy Matrix\n",
    "    print(f\"  Generating Accuracy Matrix for {domain}\")\n",
    "    cm_acc = confusion_matrix(\n",
    "        domain_full['label'], \n",
    "        domain_full['pred_label_id'], \n",
    "        labels=range(len(TICK_LABELS))\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_acc, display_labels=TICK_LABELS)\n",
    "    disp.plot(cmap='Blues', ax=ax, values_format='d', colorbar=True)\n",
    "    \n",
    "    for labels in disp.text_.ravel():\n",
    "        labels.set_color(\"black\")\n",
    "        if labels.get_text() != '0': labels.set_weight(\"bold\")\n",
    "        \n",
    "    plt.title(f'Accuracy Matrix: {domain.capitalize()} Domain')\n",
    "    plt.tight_layout()\n",
    "    filename_acc = f\"{output_folder}/Matrix_Accuracy_{domain}.png\"\n",
    "    plt.savefig(filename_acc)\n",
    "\n",
    "    # Error Heat map\n",
    "    print(f\"  Generating Error Heatmap for {domain}\")\n",
    "    \n",
    "    if len(domain_errors) > 0:\n",
    "        cm_err = confusion_matrix(\n",
    "            domain_errors['label'], \n",
    "            domain_errors['pred_label_id'], \n",
    "            labels=range(len(TICK_LABELS))\n",
    "        )\n",
    "        np.fill_diagonal(cm_err, 0)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_err, display_labels=TICK_LABELS)\n",
    "        disp.plot(cmap='Reds', ax=ax, values_format='d', colorbar=True)\n",
    "        \n",
    "        for labels in disp.text_.ravel():\n",
    "            labels.set_color(\"black\")\n",
    "            if labels.get_text() != '0': labels.set_weight(\"bold\")\n",
    "            \n",
    "        plt.title(f'Error Heatmap: {domain.capitalize()} Domain (Mistakes Only)')\n",
    "        plt.tight_layout()\n",
    "        filename_err = f\"{output_folder}/Matrix_Error_{domain}.png\"\n",
    "        plt.savefig(filename_err)\n",
    "        plt.close()\n",
    "        print(f\"  -> Saved {filename_err}\")\n",
    "    else:\n",
    "        print(\" No errors\")\n",
    "\n",
    "    #word frequency table\n",
    "    print(f\"\\n Word Analysis for {domain} \")\n",
    "    \n",
    "    actual_model_names = df_full['model'].unique()\n",
    "\n",
    "    # Deceptive Words (From Errors)\n",
    "    print(\" Top Trigger Words:\")\n",
    "    for model_name in actual_model_names:\n",
    "        subset = domain_errors[domain_errors['model'] == model_name]\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            try:\n",
    "                vectorizer = CountVectorizer(stop_words='english', max_features=10, ngram_range=(1, 2))\n",
    "                X = vectorizer.fit_transform(subset['text'])\n",
    "                \n",
    "                # Calculate counts\n",
    "                word_counts = np.asarray(X.sum(axis=0)).flatten()\n",
    "                feature_names = vectorizer.get_feature_names_out()\n",
    "                word_freq = list(zip(feature_names, word_counts))\n",
    "                word_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                formatted_list = [f\"{word} ({count})\" for word, count in word_freq]\n",
    "                print(f\"    {model_name}: {', '.join(formatted_list)}\")\n",
    "            except ValueError:\n",
    "                pass \n",
    "\n",
    "    # Full Data\n",
    "    print(\"Baseline Words (Natural vocabulary):\")\n",
    "    for model_name in actual_model_names:\n",
    "        subset = domain_full[domain_full['model'] == model_name]\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            try:\n",
    "                vectorizer = CountVectorizer(stop_words='english', max_features=10, ngram_range=(1, 2), max_df=0.9)\n",
    "                X = vectorizer.fit_transform(subset['text'])\n",
    "                \n",
    "                word_counts = np.asarray(X.sum(axis=0)).flatten()\n",
    "                feature_names = vectorizer.get_feature_names_out()\n",
    "                \n",
    "                word_freq = list(zip(feature_names, word_counts))\n",
    "                word_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                formatted_list = [f\"{word} ({count})\" for word, count in word_freq]\n",
    "                print(f\"    {model_name}: {', '.join(formatted_list)}\")\n",
    "            except ValueError:\n",
    "                pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
